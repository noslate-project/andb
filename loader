#!/bin/bash

_=[ 'exec' '/bin/sh' '-c' '''
command -v python >/dev/null && exec python "$0" "$@"
command -v python3 >/dev/null && exec python3 "$0" "$@"
exec python "$0" "$@"
''' "$0" "$@"
]
del _

import os
import sys
import signal

print(os.path.abspath(__file__))
dirname, filename = os.path.split(__file__)
dirname = os.path.expanduser(dirname)
andb_dir = os.path.abspath(dirname)

from andb.loader import FileWrap, GdbLoader, LldbLoader

import argparse

loader_desc = """
Alibaba Noslate Debugger Loader

A better experience for any node.js developers.

1) Debugging a corefile using lldb/gdb, 
    
    andb -l -c core
    andb -g -c core
    
    -l, --lldb : choose lldb for debugging (default)
    -g, --gdb  : choose gdb for debgging
    -c, --core : path to corefile 

2) Debug a live process, 
    
    andb -l -p <pid>
    
    -p, --pid : pid of living process

3) Batch debugging,

    andb -l -c core -b -xf export_snapshot.cmd
    andb -l -c core -b -x iso g p
    
    -b, --batch : batch mode
    -xf, --command-file : command from file
    -x, --command : command from line

    -x and -xf can be used multiple times and in conjunction. 

4) Export core.heapsnapshot by MapReduce,

    andb -l -c core -m snapshot -j 4

    -m  : --mode, MapReduce mode.
    -j  : --jobs, allow N jobs at once.

"""

parser = argparse.ArgumentParser(description=loader_desc, formatter_class=argparse.RawTextHelpFormatter)
parser.add_argument('-g', '--gdb', action='store_true', help='using gdb as debugger.')
parser.add_argument('-l', '--lldb', action='store_true', help='using lldb as debugger. (default)')
parser.add_argument('-p', '--pid', nargs=1, type=int, help='the process id to attach to.')
parser.add_argument('-b', '--batch', action='store_true', help='the process id to attach to.')
parser.add_argument('-t', '--tag', nargs=1, type=str, help='specified version for debugging.')
parser.add_argument('-m', '--mode', nargs=1, choices=['snapshot', 'cache'], help='mapreduce mode (snapshot, cache).') 
parser.add_argument('-j', '--jobs', action='store', type=int, help='jobs for mapreduce.')
parser.add_argument('-x', '--command', dest='cmds', action='append', nargs="+", type=str, help='eval command can be multiple times.')
parser.add_argument('-xf', '--command-file', dest='cmds', action='append', nargs=1, type=FileWrap, help='eval command can be multiple times.')
parser.add_argument('-c', '--core', nargs="?", type=str, help='path to corefile.')
parser.add_argument('--args', nargs=argparse.REMAINDER, help='debugger options')
parser.add_argument('binary', nargs="?", type=str, help='node or shinki binaray')
parser.add_argument('-z', '--tsr', action='store_true', help='Generate Technical Support Report.')

args, dbg_opts = parser.parse_known_args()

#if not andb_dir in sys.path:
#    sys.path.insert(0, andb_dir)

#print('gdb:', args.gdb, 'lldb:', args.lldb,
#  'core:', args.core, 'binary:', args.binary, 'opts:', args.opts)

# node and node.typ path
binary = None
typfile = None

def Abort():
    print("Aborted, killall sub-processes.")
    os.killpg(0, 9)

def term_handler(signo, frame):
    print('')
    print("Received Signal(%d)" % signo)
    Abort()

def GetLoader(andb_dir):
    if args.gdb:
        return GdbLoader(andb_dir)
    return LldbLoader(andb_dir)

if args.binary:
    """ use specified binary
    """
    binary = args.binary

elif args.core:
    """ only corefile
    """
    from andb.loader import CorefileAuxiliaryDownloader, Corefile
    corepath = args.core
    corefileFmt = Corefile()
    corefileFmt.Load(corepath)

    corefileAuxiliaryDownloader = CorefileAuxiliaryDownloader()
    
    if args.tag and len(args.tag) > 0:
        info = corefileAuxiliaryDownloader.FetchByTag(args.tag[0])
    else:
        buildId = corefileFmt.GetBuildId()
        print('build-id:', buildId)
        info = corefileAuxiliaryDownloader.FetchByBuildId(buildId)

    typfile = info['typ']
    os.environ['ANDB_TYP'] = typfile 

    if 'bin' in info:
        binary = info['bin']

def IndexProcess(size):
    loader = GetLoader(andb_dir)
    loader.SetExec(binary)
    loader.SetTyp(typfile)
    loader.SetCore(args.core)
    loader.BatchOn()
    loader.AddCommandFile('%s/init/pre_mapreduce.cmd'%andb_dir)
    loader.AddCommandLine('mapreduce index %d'%size)
    opts = loader.Opts()
    return os.spawnvp(os.P_WAIT, opts[0], opts)

def SnapProcess(index):
    print("my index is %d" % index)
    loader = GetLoader(andb_dir)
    loader.SetExec(binary)
    loader.SetTyp(typfile)
    loader.SetCore(args.core)
    loader.BatchOn()
    loader.AddCommandFile('%s/init/pre_mapreduce.cmd'%andb_dir)
    loader.AddCommandLine('mapreduce snapshot %d'%index)
    opts = loader.Opts()
    #os.execp(opts[0], opts)
    os.spawnvp(os.P_WAIT, opts[0], opts)

def ReduceProcess(concurrency):
    loader = GetLoader(andb_dir)
    loader.SetExec(binary)
    loader.SetTyp(typfile)
    loader.SetCore(args.core)
    loader.BatchOn()
    loader.AddCommandFile('%s/init/pre_mapreduce.cmd'%andb_dir)
    loader.AddCommandLine('mapreduce reduce')
    opts = loader.Opts()
    os.spawnvp(os.P_WAIT, opts[0], opts)
    
def MapReduce():
    from multiprocessing import Process, Pool
    from time import time
    import re

    t0 = time()
    concurrency = 4 
    if args.jobs:
        concurrency = args.jobs

    p = Process(target=IndexProcess, args=(100,))
    p.start()
    p.join()
    t1 = time()
    print("IndexProcesses done.")

    reduce_process = Process(target=ReduceProcess, args=(concurrency,))
    reduce_process.start()

    maps = [] 
    for f in os.listdir("snapshot.d"):
        if f.endswith(".map"):
            n = re.findall(r'\d+', f)[0]
            maps.append(int(n))
    maps.sort()
    print(maps)

    pool = Pool(processes=concurrency)
    results = [ pool.apply_async(SnapProcess, (i,)) for i in maps ]
    pool.close()
    pool.join()
    t2 = time()
    print("SnapshotProcesses all done.")

    failed = False
    for i in maps:
        if not os.path.exists("snapshot.d/snapshot_%d.rec" % i):
            print("Job map_%d failed." % i)
            failed = True

    if failed:
        print("ReduceProcess not satisfied.")
        Abort()
        assert 0

    print("Wait ReduceProcess to complete ...")
    reduce_process.join()
    t3 = time()

    print('real   {:.3f}s'.format(t3-t0))
    print('map    {:.3f}s'.format(t1-t0))
    print('parse  {:.3f}s'.format(t2-t1))
    print('reduce {:.3f}s'.format(t3-t2))


if __name__ == '__main__':

    # Generate Technical Support Report.
    if args.tsr:
        from andb.loader import TechReport
        if args.core:
            rpt = TechReport(corefileFmt)
            rpt.Generate("%s.tsr" % args.core)
        elif args.binary:
            TechReport.ShowTextReport(args.binary)
        exit(0)

    # map reduce mode
    if args.mode:
        # leader the new process group
        os.setpgrp()
        signal.signal(signal.SIGINT, term_handler)
        signal.signal(signal.SIGTERM, term_handler)

        MapReduce()
        exit(0)

    if binary is None and \
        args.core is None and \
        args.pid is None:
        exit(0)

    # single process mode
    loader = GetLoader(andb_dir)
    if binary: 
        loader.SetExec(binary)
    if args.core:
        loader.SetCore(args.core)
    if args.pid:
        loader.SetPid(args.pid[0])
    if args.batch:
        loader.BatchOn()
    if args.args and len(args.args) > 0:
        loader.AddArgs(args.args)
    if args.cmds and len(args.cmds) > 0:
        print(args.cmds)
        loader.AddCommands(args.cmds)

    # ignore CTRL+C for python
    signal.signal(signal.SIGINT, signal.SIG_IGN)

    # spawn dbg
    opts = loader.Opts() 
    os.spawnvp(os.P_WAIT, opts[0], opts)

